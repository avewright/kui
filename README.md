# ğŸš€ Simple vLLM Document Extraction

A clean, modern document extraction system with vLLM vision models and React.

## ğŸ“ Project Structure (Simple!)

```
â”œâ”€â”€ api/                 # Backend FastAPI server
â”‚   â”œâ”€â”€ main.py         # Main API code
â”‚   â”œâ”€â”€ requirements.txt # Python dependencies
â”‚   â””â”€â”€ Dockerfile      # For Docker deployment
â”œâ”€â”€ frontend/            # React web interface
â”‚   â”œâ”€â”€ src/            # React components & code
â”‚   â”œâ”€â”€ package.json    # Node.js dependencies
â”‚   â””â”€â”€ Dockerfile      # For Docker deployment
â”œâ”€â”€ vllm/               # Vision model deployment
â”‚   â”œâ”€â”€ deploy.py       # Deploy vision models
â”‚   â””â”€â”€ config.yaml     # Model configuration
â”œâ”€â”€ scripts/            # Easy deployment scripts
â”‚   â”œâ”€â”€ setup.sh        # One-time setup
â”‚   â””â”€â”€ start.sh        # Start everything
â”œâ”€â”€ docker-compose.yml  # Run everything with Docker
â””â”€â”€ README.md          # This file
```

**That's it!** Only 4 main folders you need to care about.

## ğŸ¯ What Each Folder Does

| Folder | Purpose | You Need This If... |
|--------|---------|---------------------|
| `api/` | Handles PDF processing and talks to AI models | Always (this is the backend) |
| `frontend/` | The website you see in your browser | Always (this is the UI) |
| `vllm/` | Deploys AI vision models for document reading | You want to run AI models locally |
| `scripts/` | Makes setup and starting easy | You want simple commands |

## ğŸš€ Super Easy Deployment

### Option 1: Automatic Setup (Recommended)
```bash
# Setup everything once
./scripts/setup.sh

# Start everything
./scripts/start.sh
```
**Done!** Go to http://localhost:3000

### Option 2: Docker (Even Easier)
```bash
# Start everything with one command
docker-compose up
```
**Done!** Go to http://localhost:3000

### Option 3: Manual (If you want control)
```bash
# 1. Start the AI model server
cd vllm
python deploy.py --model llava-1.5-7b

# 2. Start the backend (new terminal)
cd api
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
python main.py

# 3. Start the frontend (new terminal)
cd frontend
npm install
npm run dev
```

## ğŸ® How to Use

1. **Open your browser** â†’ http://localhost:3000
2. **Upload a PDF** â†’ Drag & drop or click to select
3. **Configure fields** â†’ Tell it what to extract (title, date, etc.)
4. **Click Process** â†’ Watch it extract data using AI
5. **Download results** â†’ Get JSON with extracted data

## ğŸ”§ Configuration

### Change AI Model
Edit `vllm/config.yaml`:
```yaml
model: "llava-hf/llava-1.5-13b-hf"  # Use bigger model
gpu_memory_utilization: 0.95        # Use more GPU memory
```

### Change Ports
Edit `api/.env`:
```
VLLM_ENDPOINT=http://localhost:8000
API_PORT=8080
```

Edit `frontend/.env`:
```
REACT_APP_API_URL=http://localhost:8080
```

## ğŸ› ï¸ Requirements

- **Python 3.8+** (for AI models)
- **Node.js 18+** (for web interface)
- **GPU** (optional, for local AI models)
- **Docker** (optional, for easy deployment)

## â“ Need Help?

**Model won't start?** Check if you have enough GPU memory (8GB+ recommended)

**Can't access website?** Make sure ports 3000, 8080, and 8000 aren't in use

**Extraction not working?** Check that vLLM server is running at http://localhost:8000

**Want different model?** Run: `python vllm/deploy.py --list-models`

## ğŸ”¥ Features

- **Drag & drop PDFs** - Easy file upload
- **Custom fields** - Extract exactly what you need
- **Real-time processing** - See progress as it works
- **Multiple models** - Support for LLaVA, MiniCPM-V, Qwen-VL
- **Export results** - Download as JSON
- **Cloud ready** - Deploy anywhere

---

**That's it!** Simple, clean, and powerful document extraction with AI. 